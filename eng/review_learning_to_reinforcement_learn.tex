\documentclass[12pt,a4paper]{article}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{geometry}
\usepackage{indentfirst}
\usepackage{pifont}
\usepackage{ulem}

\usepackage{algorithm} 
\usepackage{algorithmicx} 
\usepackage{algpseudocode}  
\usepackage{amsmath}  
\renewcommand{\algorithmicrequire}{\textbf{Input:}}  % Use Input in the format of Algorithm  
\renewcommand{\algorithmicensure}{\textbf{Output:}} % Use Output in the format of Algorithm  

\setlength{\parindent}{0em}
\geometry{left=2.0cm,right=2.0cm,top=2.5cm,bottom=2.5cm}

\begin{document}
\title{Learning to Reinforcement Learn\cite{Wang2016Learning}}
\author{esgl Hu}
\maketitle
\paragraph{} Recent advances have allowed long-standing methods for reinforcement learning (RL) to be newly extended to such complex and large-scale task environments as Atari and Go. \uline{The key enabling breakthrough has been the development of techniques allowing the stable integration of RL with non-linear function approximation through deep learning.} The resulting deep RL methods are attaining human- and often superhuman-level performance in an expanding list of domains. However, there are at least two aspects of human performance that they starkly lack. 
\begin{itemize}
	\item \textbf{\uline{First, deep RL typically requires a massive volume of training data, whereas human learners can attain reasonable performance on any of a wide range of tasks with comparatively title experience.}}
	\item \textbf{\uline{Second, deep RL systems typically specialize on one restricted task domain, whereas human learnerscan flexibly adapt to changing task conditions.}}
\end{itemize}
\bibliographystyle{plain}
\bibliography{reference}
\end{document}





































