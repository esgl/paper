\documentclass[12pt,a4paper]{article}
\def\allfiles{}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{geometry}
\usepackage{indentfirst}
\usepackage{pifont}
\usepackage{ulem}
\usepackage{color}
\usepackage{algorithm} 
\usepackage{algorithmicx} 
\usepackage{algpseudocode}  
\usepackage{amsmath}  
\usepackage{bm}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}  % Use Input in the format of Algorithm  
\renewcommand{\algorithmicensure}{\textbf{Output:}} % Use Output in the format of Algorithm  

\setlength{\parindent}{0em}
\geometry{left=2.0cm,right=2.0cm,top=2.5cm,bottom=2.5cm}

\begin{document}
\title{Algorithms in Reinforcement Learning}
\author{Guannan Hu}
\maketitle

\begin{algorithm}
\caption{Semi-gradient TD($\lambda$) for estimating $\hat{v} \approx v_{\pi}$}
\begin{algorithmic}
	\State Input: the policy $\pi$ to be evaluated
	\State Input: a differentiable function $\hat{v}: \mathcal{S}^{+} \times \mathbb{R}^{d} \rightarrow \mathbb{R}$ such that $\hat{v}(terminal, \cdot) = 0$
	\State Algorithm parameters: step size $\alpha > 0$, trace decay rate $\gamma \in [0, 1]$
	\State Initialize value-function weights $\mathbf{w}$ arbitrarily (e.g., $\mathbf{w} = \mathbf{0}$)
	\State
	\Loop\ for each episode:
		\State Initialize $S$
		\State $\mathbf{z} \leftarrow \mathbf{0}$
		\Repeat\ for each step of episode:
			\State Choose $A \sim \pi(\cdot|S)$
			\State Take action $A$, observe $R, S^{'}$
			\State $\mathbf{z} \leftarrow \gamma\lambda\mathbf{z} + \nabla\hat{v}(S,\mathbf{w})$
			\State $\delta \leftarrow R + \gamma\hat{v}(S^{'}, \mathbf{w}) - \hat{v}(S, \mathbf{w})$
			\State $\mathbf{w} \leftarrow \mathbf{w} + \alpha\delta\mathbf{z}$
			\State $S \leftarrow S^{'}$
		\Until $S^{'}$ is terminal
		
	\EndLoop
	
\end{algorithmic}
\end{algorithm}

\newpage

\begin{algorithm}
\caption{True Online $TD(\lambda)$ for estimating $\mathbf{w}^{T}\mathbf{x} \approx v_{\pi}$}
\begin{algorithmic}
	\State Input: the policy $\pi$ to be evaluated
 	\State Input: a feature function $\mathbf{x}:\mathcal{S}^{+} \leftarrow \mathbb{R}^{d}$ such that $\mathbf{x}(terminal, \cdot) = \mathbf{0}$
	\State Algorithm parameters: step size $\alpha > 0$, trace decay rate $\gamma \in [0, 1]$
	\State Initialize value-function weights $\mathbf{w} \in \mathbb{R}^{d}$ (e.g., $\mathbf{w} = \mathbf{0}$)
	\State
	\Loop\ for each episode:
		\State Initialize state and obtain initial feature vector $\mathbf{x}$
		\State $\mathbf{z} \leftarrow \mathbf{0}$
		\State $V_{old} \leftarrow 0$
		\Repeat\ for each step of episode:
			\State Choose $A \sim \pi$
			\State Take action $A$, observe $R, \mathbf{x}^{'}$ (feature vector of the next state)
			\State $V \leftarrow \mathbf{w}^{T}\mathbf{x}$
			\State $V^{'} \leftarrow \mathbf{w}^{T}\mathbf{x}$
			\State $\delta \leftarrow R + \gamma V^{'} - V$
			\State $\mathbf{z} \leftarrow \gamma\lambda\mathbf{z} + (1 - \alpha\gamma\lambda\mathbf{z}^{T}\mathbf{x})\mathbf{x}$
			\State $\mathbf{w} \leftarrow \mathbf{w} + \alpha(\delta + V - V_{old})\mathbf{z} - \alpha(V - V_{old})\mathbf{x}$
			\State $V_{old} \leftarrow V^{'}$
			\State $\mathbf{x} \leftarrow \mathbf{x}^{'}$	
		\Until {$\mathbf{x}^{'} = \mathbf{0}$ (signaling arrival at the terminal state)}		
			
	\EndLoop
\end{algorithmic}
\end{algorithm}

\newpage

\begin{algorithm}
\caption{Sarsa($\lambda$) with binary features and linear function approximation for estimating $\mathbf{w}^{T}\mathbf{x} \approx q_{\pi}$ or $q_{*}$}
\begin{algorithmic}
	\State Input: a function $\mathcal{F}(s, a)$ returning the set of (indices of) active features for $s,a$
	\State Input: a policy $\pi$ (if estimating $q_{\pi}$)
	\State Algorithm parameters: step size $\alpha > 0$, trace decay rate $\lambda \in [0, 1]$
	\State Initialize: $\mathbf{w}=(w_1, ..., w_d)^{T} \in \mathbb{R}^{d}$ (e.g., $\mathbf{w} = \mathbf{0}$), $\mathbf{z}=(z_1, ..., z_d)^{T} \in \mathbb{R}^{d}$
	\State
	\Loop\ for each episode:
		\State Initialize $S$
		\State Choose $A \sim \pi(\cdot|S)$ or $\epsilon$-greedy according to $\hat{q}(S, \cdot, \mathbf{w})$
		\State $\mathbf{z} \leftarrow \mathbf{0}$
		\Loop\ for each step of episode:
			\State Take action $A$, observe $R, S^{'}$
			\State $\delta \leftarrow R$
			\Loop\ for $i$ in $\mathcal{F}(S, A)$
				\State $\delta \leftarrow \delta - w_{i}$
				\State Set $$z \leftarrow \left\{
				\begin{array}{ll}
				z_{i}+1 & \text{(accumulating traces)}\\
				1 &  \text{(replaceing traces)}				
				\end{array}\right.
				$$
			\EndLoop
			\If {$S^{'}$ is terminal then}
				\State $\mathbf{w} \leftarrow \mathbf{w} + \alpha \delta \mathbf{z}$
				\State Go to next episode
			\EndIf
			\State Choose $A^{'} \sim \pi(\cdot|S^{'})$ or near greedily $\sim \hat{q}(S^{'}, \cdot, \mathbf{w})$
			\Loop\ for $i$ in $\mathcal{F}(S^{'}, A^{'})$:
				\State $\delta \leftarrow \delta + \gamma w_{i}$
			\EndLoop
			\State $\mathbf{w} \leftarrow \mathbf{w} + \alpha\delta\mathbf{z}$
			\State $\mathbf{z} \leftarrow \gamma\lambda\mathbf{z}$
			\State $S \leftarrow S^{'}; A \leftarrow A^{'}$
			
		\EndLoop
	\EndLoop
\end{algorithmic}
\end{algorithm}

\newpage

\begin{algorithm}
\caption{True Online Sarsa($\lambda$) for estimating $\mathbf{w}^{T}\mathbf{x} \approx q_{\pi}$ or $q_{*}$}
\begin{algorithmic}
	\State Input: a feature function $\mathbf{x}: \mathcal{S}^{+} \times \mathcal{A} \leftarrow \mathbb{R}^{d} $ such that $\mathbf{x}(terminal, \cdot)=\mathbf{0}$
	\State Input: a policy $\pi$ (if estimating $q_{\pi}$)
	\State Algorithm parameters: step size $\alpha > 0$, trace decay rate $\lambda \in [0, 1]$
	\State Initialize: $\mathbf{w} \in \mathbb{R}^d$ (e.g., $\mathbf{w} = \mathbf{0}$)
	\State
	\Loop\ for each episode:
		\State Initialize $S$
		\State Choose $A \sim \pi(\cdot|S)$ or near greedily from $S$ using $\mathbf{w}$
		\State $\mathbf{w} \leftarrow \mathbf{x}(S,A)$
		\State $\mathbf{z} \leftarrow \mathbf{0}$
		\State $Q_{old} \leftarrow 0$
		\Repeat\ {for each step of episode}:
			\State Take action $A$, observe $R, S^{'}$
			\State Choose $A^{'} \sim \pi(\cdot|S^{'})$ or near greedily from $S^{'}$ using $\mathbf{w}$
			\State $\mathbf{x}^{'} \leftarrow \mathbf{x}(S^{'}, A^{'}) $
			\State $Q \leftarrow \mathbf{w}^{T}\mathbf{x}$
			\State $Q^{'} \leftarrow \mathbf{w}^{T}\mathbf{x}^{'}$
			\State $\delta \leftarrow R + \gamma Q^{'} - Q$
			\State $\mathbf{z} \leftarrow \gamma\lambda\mathbf{z} + (1 - \alpha\gamma\lambda\mathbf{z}^{T}\mathbf{x})\mathbf{x}$
			\State $\mathbf{w} \leftarrow \mathbf{w} + \alpha(\delta + Q - Q_{old})\mathbf{z} - \alpha(Q - Q_{old})\mathbf{x}$
			\State $Q_old \leftarrow Q^{'}$
			\State $\mathbf{x} \leftarrow \mathbf{x}^{'}$
			\State $A \leftarrow A^{'}$
		\Until {$S^{'}$ is terminal}
	\EndLoop
\end{algorithmic}
\end{algorithm}

\newpage

\begin{algorithm}
\caption{REINFORCE: Monte-Carlo Policy-Gradient Method (episodic) estimating $\pi_{\bm{\theta}} \approx \pi_{*}$}
\begin{algorithmic}
	\State Input: a differential policy parameterization $\pi(a|s,\bm{\theta})$
	\State Algorithm parameter: step size $\alpha > 0$
	\State Initializa policy parameter $\bm{\theta} \in \mathbb{R}^{d^{'}}$ (e.g., to $\mathbf{0}$)
	\State
	\Loop\ forever (for each episode):
		\State Generate an episode $S_0, A_0, R_1, ..., S_{T-1}, A_{T-1}, R_T$, following $\pi(\cdot|\cdot,\bm{\theta})$
		\Loop\ for each step of the episode $t=0,1,...,T-1$:
			\State $G \leftarrow \sum_{k=t+1}^{T}R_{k}$
			\State $\bm{\theta} \leftarrow \bm{\theta} + \alpha\gamma^{t}G\nabla\text{ln}\pi(A_t|S_t,\bm{\theta})$		
		\EndLoop
	\EndLoop
\end{algorithmic}
\end{algorithm}

\newpage

\begin{algorithm}
\caption{REINFORCE with Baseline (episodic), for estimating $\pi_{\bm{\theta}} \approx \pi_{*}$}
\begin{algorithmic}
	\State Input: a differentiable policy parameterization $\pi(a|s, \bm{\theta})$
	\State Input: a differentiable state-value function parameterization $\hat{v}(s, \mathbf{w})$
	\State Algorithm parameters: step sizes $\alpha^{\bm{\theta}} > 0, \alpha^{\mathbf{w}} > 0$
	\State Initialize policy parameter $\bm{\theta} \in \mathbb{R}^{d^{'}}$ and state-value weights $\mathbf{w} \in \mathbb{R}^{d}$ (e.g., to $\mathbf{0}$)
	\State
	\Loop\ forever (for each episode):
		\State Generate an episode $S_0, A_0, R_1,..., S_{T-1}, A_{T-1}, R_{T}$, following $\pi(\cdot|\cdot, \bm{\theta})$
		\Loop\ for each step of episode $t=0,1,...,T-1$:
			\State $G \leftarrow \sum_{k=t+1}^{T}R_{k}$  \qquad $(G_t)$
			\State $\delta \leftarrow G - \hat{v}(S_{t}, \mathbf{w})$
			\State $\mathbf{w} \leftarrow \mathbf{w} + \alpha^{\mathbf{w}}\gamma^{t}\nabla\hat{v}(S_{t}, \mathbf{w})$
			\State $\bm{\theta} \leftarrow \bm{\theta} + \alpha^{\bm{\theta}}\gamma^{t}\delta\nabla\text{ln}\pi(A_t|S_{t},\bm{\theta})$
		\EndLoop
	
	\EndLoop
\end{algorithmic}
\end{algorithm}

\newpage

\begin{algorithm}
\caption{One-step Actor Critic (episodic), for estimating $\pi_{\bm{\theta}} \approx \pi_{*}$}
\begin{algorithmic}
	\State Input: a differentiable policy parameter $\pi(a|s, \bm{\theta})$
	\State Input: a differentiable state-value function parameterization $\hat{v}(s, \mathbf{w})$
	\State Algorithm parameter: step size $\alpha^{\bm{\theta}} > 0, \alpha^{\mathbf{w}} > 0$
	\State Initialize policy parameter $\bm{\theta} \in \mathbb{R}^{d^{'}}$ and state-value weights $\mathbf{w} \in \mathbb{R}^{d}$ (e.g., to $\mathbf{0}$)
	\State
	\Loop\ forever (for each episode):
		\State Initialize $S$ (first state of episode)
		\State $I \leftarrow 1$
		\Loop\ while $S$ is not terminal (for each time step):
			\State $A \sim \pi(\cdot|S, \bm{\theta})$
			\State Take action $A$, observe $S^{'}, R$
			\State $\delta \leftarrow R + \gamma\hat{v}(S^{'}, \mathbf{w}) - \hat{v}(S,\mathbf{w})$    \qquad (if $S^{'}$ is terminal, then $\hat{v}(S^{'}, \mathbf{w}) \doteq 0$)
			\State $\mathbf{w} \leftarrow \mathbf{w} + \alpha^{\mathbf{w}}I\delta\nabla\hat{v}(S,\mathbf{w})$
			\State $\bm{\theta} \leftarrow \bm{\theta} + \alpha^{\bm{\theta}}I\delta\nabla\text{ln}\pi(A|S,\bm{\theta})$
			\State $I \leftarrow \gamma I$
			\State $S \leftarrow S^{'}$
		\EndLoop
	\EndLoop
	
\end{algorithmic}
\end{algorithm}

\newpage

\begin{algorithm}
\caption{Actor-Critic with Eligibility Traces (episodic), for estimating $\pi_{\bm{\theta}} \approx \pi_{*}$}
\begin{algorithmic}
	\State Input: a differentiable policy parameterization $\hat{v}(s, \mathbf{w})$
	\State Input: a differentiable state-value function parameterization $\hat{v}(s, \mathbf{w})$
	\State Algorithm parameters: trace-decay rate $\lambda^{\bm{\theta}} \in [0, 1], \lambda^{\mathbf{w}} \in [0,1]$; step sizes $\alpha^{\bm{\theta}}>0, \alpha^{\mathbf{w}} > 0$
	\State Initialize policy parameter $\bm{\theta} \in \mathbb{R}^{d^{'}}$ and state-value weights $\mathbf{w} \in \mathbb{R}^{d}$ (e.g., to $\mathbf{0}$)
	\State	
	\Loop\ forever (for each episode):
		\State initialize $S$ (first state of episode)
		\State $\mathbf{z}^{\bm{\theta}} \leftarrow \mathbf{0}$ ($d^{'}$-component eligibility trace vector)
		\State $\mathbf{z}^{\mathbf{w}} \leftarrow \mathbf{0}$ ($d^{'}$-component eligibility trace vector)
		\State $I \leftarrow 1$
		\Loop\ while $S$ is not terminal (for each time step):
			\State $A \in \pi(\cdot|S, \bm{\theta})$
			\State Take action $A$, observe $S^{'}, R$
			\State $\delta \leftarrow \gamma\hat{v}(S^{'}, \mathbf{w}) - \hat{v}(S, \mathbf{w})$  \qquad  (if $S^{'}$ is terminal, then $\hat{v}(S^{'}, \mathbf{w})$)
			\State $\mathbf{z}^{\mathbf{w}} \leftarrow \gamma\lambda^{\mathbf{w}}\mathbf{z}^{\mathbf{w}} + I\nabla\hat{v}(s, \mathbf{w})$
			\State $\mathbf{z}^{\bm{\theta}} \leftarrow \gamma\lambda^{\bm{\theta}}\mathbf{z}^{\bm{\theta}} + I\nabla\hat{v}(s, \bm{\theta})$
			\State $\mathbf{w} \leftarrow \mathbf{w} + \alpha^{\mathbf{w}\delta\mathbf{z}^{\mathbf{w}}}$
			\State $\bm{\theta} \leftarrow \bm{\theta} + \alpha^{\mathbf{\theta}}\delta\mathbf{z}^{\bm{\theta}}$
			\State $I \leftarrow \gamma I$
			\State $S \leftarrow S^{'}$
		\EndLoop
	\EndLoop
\end{algorithmic}
\end{algorithm}

\newpage

\begin{algorithm}
	\caption{Actor-Critic with Eligibility Traces (continuing), for estimating $\pi_{\bm{\theta}} \approx \pi_{*}$}
	\begin{algorithmic}                                                                                                   
		\State Input: a differentiable policy parameterization $\pi(a|s, \bm{\theta})$
		\State Input: a differentiable state-value function parameterization $\hat{v}(s, \mathbf{w})$
		\State Algorithm parameters: trace-decay rates $\lambda^{\bm{\theta}} \in [0, 1], \lambda^{\mathbf{w}} \in [0,1]$; step sizes $\alpha^{\bm{\theta}} > 0, \alpha^{\mathbf{w}} > 0, \eta > 0$
		\State Initialize $\bar{R} \in \mathbb{R}$ (e.g., to 0)
		\State Initialize policy parameter $\bm{\theta} \in \mathbb{R}^{d^{'}}$ and state-value weights $\mathbf{w} \in \mathbb{R}^{d}$ (e.g., to $\mathbf{0}) \doteq 0$
		\State
		\Loop\ forever (for each time step):
			\State $A \sim \pi(\cdot|S, \bm{\theta})$
			\State Take Action $A$, observe $S^{'}, R$
			\State $\delta \leftarrow R - \bar{R} + \hat{v}(S^{'}, \mathbf{w}) - \hat{v}(S, \mathbf{w})$
			\State $\bar{R} \leftarrow \bar{R} + \eta\delta$
			\State $\mathbf{z}^{\mathbf{w}} \leftarrow \lambda^{\mathbf{w}}\mathbf{z}^{\mathbf{w}} + \nabla\hat{v}(S, \mathbf{w})$
			\State $\mathbf{z}^{\bm{\theta}} \leftarrow \lambda^{\bm{\theta}}\mathbf{z}^{\bm{\theta}} + \nabla \text{ln}\pi(A|S, \bm{\theta})$
			\State $\mathbf{w} \leftarrow \mathbf{w} + \alpha^{\mathbf{w}}\delta\mathbf{z}^{\mathbf{w}}$
			\State $\mathbf{w} \leftarrow \bm{\theta} + \alpha^{\bm{\theta}}\delta\mathbf{z}^{\bm{\theta}}$
			\State $S \leftarrow S^{'}$
		\EndLoop
	\end{algorithmic}
\end{algorithm} 

\newpage
\begin{algorithm}
\caption{Double DQN with proportional prioritization}
\begin{algorithmic}
	\State Input: minibatch $k$, step-size $\eta$, replay period $K$, and size $N$, exponents $\alpha$ and $\beta$, budget $T$.
	\State Initialize replay memory $\mathcal{H} = \Phi, \Delta = 0, p_{1}=1$
	\State Observe $S_{0}$ and choose $A_{0} \sim \pi_{\theta}(S_{0})$
	\For {$t=1 \to T$}
		\State observe $S_{t}, R_{t}, \gamma_{t}$
		\State Store transition $(S_{t-1}, A_{t-1}, R_{t}, \gamma_{t}, S_{t})$ with $\mathcal{H}$ with maximal priority $p_{t} = \max_{i}p_{i}$
		\If {$t \equiv 0 \mod K$}
			\For {$j= 1 \to k$}
				\State Sample transition $j \sim P(j) = \frac{p_{j}^{\alpha}}{\max_{i}w_{i}}$
				\State Compute importance-sampling weight $w_{j} = \frac{(N \cdot P(j))^{-\beta}}{\max_{i}w_{i}}$
				\State Compute TD-error $\delta_{j} = R_{i} +\gamma_{j}Q_{target}(S_{j}, \text{argmax}_{a}Q(S_{j}, a)) - Q(S_{j-1}, A_{j-1})$
				\State Update transition priority $p_{j} \leftarrow |\delta_{j}|$
				\State Accumulate weight change $\Delta \leftarrow \Delta + w_{i}\cdot\delta_{j}\cdot\nabla_{\theta}Q(S_{j-1}, A_{j-1})$
			\EndFor
			\State Update weights $\theta \leftarrow \theta + \eta \cdot \Delta$, reset $\Delta = 0$
			\State From time to time copy weights into target network $\theta_{target}\leftarrow \theta$
		\EndIf
		\State Choose action $A_{t} \sim \pi_{\theta}(S_{t})$
	\EndFor
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Distributed Prioritized Experience Replay}
\begin{algorithmic}

\Procedure {Actor($B, T$)}{} % \quad $\triangleright$ Run agent in environment instance, storing experiences.
	\State $\theta_{0} \leftarrow$ LEARNER.PARAMETERS() % \quad $\triangleright$ Remote call to obtain latest network parameters.
	\State $s_0 \leftarrow$ ENVIRONMENT.INITIALIZE()  % \quad $\triangleright$ Get initial state from environment.
	\For {$t = 1 \to T$}
		\State $a_{t-1} \leftarrow \pi_{\theta}(s_{t-1})$ %  \quad $\triangleright$ Select an action using the current policy.
		\State $(r_t, \gamma_t, s_{t}) \leftarrow $ ENVIRONMENT.STEP($a_{t-1}$) % \quad $\triangleright$ Apply the action in the environment.
		\State LOCALBUFFER.ADD($(s_{t-1}, a_{t-1}, r_t, \gamma_t)$) % \quad $\triangleright$ Add data to local buffer.
		\If {LOCALBUFFER.SIZE() $\geq B$ } % \quad $\triangleright$ In a background thread, periodically send data to replay
			\State $\tau \leftarrow$ LOCALBUFFER.GET($B$) % \quad $\triangleright$ Get buffered data (e.g. batch of multi-step transitions).
			\State $p \leftarrow$ COMPUTERPRIORITIES($\tau$) % \quad $\triangleright$  Calculate priorities for experience (e.g. absolute TD error)
			\State REPLAY.ADD($\tau, p$) % \quad $\triangleright$ Remote call to add experience to replay memory.
		\EndIf
		\State PERIODICALLY($\theta_{t} \leftarrow$ LEARNER.PARAMETERSOP()) % \quad $\triangleright$ Obtain latest network parameters. 
	\EndFor
\EndProcedure
\State
\Procedure {Learner({T})}{}
	\State $\theta_{0} \leftarrow$ INITIALIZENETWORK() % \quad $\triangleright$ Update network using batches sampled from memory.
	\For {$t=0 \to T$}
		\State $id, \tau \leftarrow $ REPLAY.SAMPLE() % \quad $\triangleright$ Sample a prioritized batch of transitions (in a background thread).
		\State $l_{t} \leftarrow $ COMPUTELOSS($\tau; \theta_{t}$) % \quad $\triangleright$ Apply learning rule; e.g. double Q-learning or DDPG.
		\State $\theta_{t+1} \leftarrow$ UPDATEPARAMETERS($l_{t};\theta_{t}$)
		\State $p \leftarrow$ COMPUTEPRIORITIES() % \quad $\triangleright$ Calculate priorities for experience, (e.g. absolute TD error).
		\State REPLAY.SETPRIORITY($id, p$) % \quad $\triangleright$ Remote call to update priorities
		\State PERIODICALLY(REPLAY.REMOVETOFIT()) % \quad $\triangleright$ Remove old experience from replay memory.
	\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

\bibliographystyle{plain}
\bibliography{reference}
\end{document}
\newpage