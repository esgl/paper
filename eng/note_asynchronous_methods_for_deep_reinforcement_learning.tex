\documentclass[12pt,a4paper]{article}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{geometry}
\usepackage{indentfirst}
\usepackage{pifont}
\usepackage{ulem}
\usepackage{color}
\usepackage{algorithm} 
\usepackage{algorithmicx} 
\usepackage{algpseudocode}  
\usepackage{amsmath}  
\renewcommand{\algorithmicrequire}{\textbf{Input:}}  % Use Input in the format of Algorithm  
\renewcommand{\algorithmicensure}{\textbf{Output:}} % Use Output in the format of Algorithm  

\setlength{\parindent}{0em}
\geometry{left=2.0cm,right=2.0cm,top=2.5cm,bottom=2.5cm}

\begin{document}
\title{Notes: Asynchronous Methods for Deep Reinforcement Learning\cite{MnihBMGLHSK16}}
\author{esgl Hu}
\maketitle
\section{Review}
\paragraph{}The simple online RL algorithms with deep neural networks was fundamentally \uline{unstable}.
\paragraph{}The sequence of observed data encountered by an online RL agent is non-stationary, and online RL updates are strongly correlated.
\paragraph{}\textbf{The \textcolor{red}{experience replay} has several \textcolor{blue}{drawbacks}}: \uline{it uses more memory and computation per real interaction; and it requires off-policy learning algorithms that can update from data generated by an older policy.}
\paragraph{} The loss function of one-step Q-learning is 
\begin{equation}\label{loss_function_one_step_q_learning}
L_{i}(\theta_{i}) = \mathbb{E}(r + \gamma\cdot max_{a^{'}}Q(s^{'}, a^{'}; \theta_{i-1}) - Q(s, a; \theta_{i}))^{2}
\end{equation}
where $s^{'}$ is the state encounted after state $s$. \uline{One drawback of using one-step method is that obtaining \textcolor{red}{a reward $r$ only directly affects the value of the state action pair $s, a$ }that led to the reward. The values of other state action pairs are affected only indirectly througt the updated value $Q(s, a)$. This can make the learning process slow since many updates are required the propagate a reward to the relevant preceding states and actions.} One way of propagating rewards faster is by using $n$-step returns. In $n$-step Q-learning, $Q(s, a)$ is updated toward the $n$-step return defined as \uline{$r_{t} + \gamma r_{t+1} + \cdots + \gamma^{n-1}r_{t+n-1} + \text{max}_{a}\gamma^{n}Q(s_{t+n}, a)$}.

\paragraph{}Policy-based model-free methods directly parameterize the policy $\pi(a|s;\theta)$ and update the parameters $\theta$ by performing, typically approximate, gradient ascent on $\mathbb{E}[R_{t}]$. For example, standard \textbf{REINFORCE} updates the policy parameters $\theta$ in the direction $\nabla_{\theta}\text{log}\pi(a_{t}|s_{t}; \theta)R_{t}$, which is an unbiased estimate of $\nabla_{\theta}\mathbb{E}[R_{t}]$. It is possible to reduce the variance of this estimate while keeping it unbiased by substracting a learned function of the state $b_{t}(s_{t})$, known as a baseline from the return. The resulting gradient is \uline{$\nabla_{\theta}long\pi(a_{t}|s_{t}; \theta)(R_{t} - b_{t}(s_{t}))$}.

\section{Algorithm}
\begin{algorithm}
	\caption{Asychronous one-step Q-learning - pseudocode for each actor-learner thread.}
	\begin{algorithmic}[1]
		\State \textit{// Assume global shared $\theta, \theta^{-}$, and counter $T = 0$.}
		\State Initialize thread step counter $t \leftarrow 0$
		\State Initialize target network weights $\theta^{-} \leftarrow \theta$
		\State Initialize network gradients $d\theta \leftarrow 0$
		\State Get initial state $s$
		\Repeat
			\State Take action $a$ with $\epsilon$-greedy policy based on $Q(s, a; \theta)$
			\State Recieve new state $s^{'}$ and reward $r$
			\State $y = \left\lbrace
				\begin{array}{lcl}
					r & & \text{for terminal }s^{'} \\
					r + \gamma max_{a^{'}}Q(s^{'}, a^{'}; \theta^{-}) & & \text{for non-terminal }s^{'}
				\end{array}	\right.$						
			
			\State Accumulate gradients wrt : $d\theta \leftarrow d\theta + \frac{\partial(y - Q(s, a; \theta))^{2}}{\partial\theta}$
			\State $s = s^{'}$
			\State $T \leftarrow T + 1$ and $t \leftarrow t + 1$
			\If {$T$ mod $I_{target} == 0$}
				\State Update the target network $\theta^{-} \leftarrow \theta$
			\EndIf
			\If{ $t$ mod $I_{asyncUpdate} == 0$ or $s$ is terminal}
				\State Perform asynchronous update of $\theta$ using $d\theta$.
				\State Clear gradients $d\theta \leftarrow 0$.
			\EndIf
		\Until{$T > T_{max}$}
	\end{algorithmic}
\end{algorithm}

\bibliographystyle{plain}
\bibliography{reference}
\end{document}