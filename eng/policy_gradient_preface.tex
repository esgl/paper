\documentclass[12pt,a4paper]{article}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{geometry}
\usepackage{indentfirst}
\setlength{\parindent}{0em}
\usepackage{pifont}
\usepackage{ulem}

\usepackage{algorithm} 
\usepackage{algorithmicx} 
\usepackage{algpseudocode}  
\usepackage{amsmath}  
\renewcommand{\algorithmicrequire}{\textbf{Input:}}  % Use Input in the format of Algorithm  
\renewcommand{\algorithmicensure}{\textbf{Output:}} % Use Output in the format of Algorithm  

\setlength{\parindent}{0em}

\geometry{left=2.0cm,right=2.0cm,top=2.5cm,bottom=2.5cm}

\begin{document}
\title{Policy Gradient Methods}
\author{esgl Hu}
\maketitle
\section{preface}

\paragraph{Notations \& Definitions}
\subparagraph{}The methods have learned the values of actions and then selected actions based on their estimated action values, their policies would not even exist without the action-value estimates.

\subparagraph{}The methods have learned a \textit{parameterized policy} that can select actions without consulting a value function. A value function may still be used to \textit{learn} the policy weights, but is not required for action selection.

\subparagraph{}The notation $\theta \in \mathbb{R}^{n}$ is used for the primary learned weigth vector, $\pi (a|s, \theta) \doteq \text{Pr}\{A_{t}=a | S_{t} = s, {\theta}_{t} = \theta\}$ for the probability that action \textit{a} is taken at time \textit{t} given that the agent is in state \textit{s} at time \textit{t} weight vector \textbf{$\theta$}.

\subparagraph{}These methods seek to maximize preformance $\eta(\theta)$, so their updates approximate gradient ascent in $\eta$:
\begin{equation}
\theta_{t+1} \doteq \theta_{t} + \alpha \widehat{\nabla(\eta(\theta_{t})})
\end{equation}
where $\widehat{\nabla(\eta(\theta_{t}))}$ is a stochastic estimate whose expectation approximates the gradients of the performance measure $\eta(\theta_{t})$ with respect to its argument $\theta_{t}$
\paragraph{Advantages \& Disadvantages}
\subparagraph{Advantages}
\begin{itemize}
\item Better convergence properties
\item Effective in high-dimensional or continuous action spaces
\item Can learn stochastic policies
\end{itemize}
\subparagraph{Disadvantages}
\begin{itemize}
\item Typically converge to a local rather than global optimum
\item Evaluating a policy is typically inefficient and high variance
\end{itemize}

\paragraph{Definition of Policy Gradient}
\subparagraph{}We let $\tau$ denote a state-action sequence $\{s_0, a_0, ..., s_H, a_H\}$. We overload nototation: $R(\tau) = \sum_{t=0}^{H} R(s_t, a_t)$.
\begin{equation}
\eta (\theta) = \mathbb{E}[\sum_{t=0}^{H}R(s_t, a_t), \pi_{\theta}] = \sum_{\tau}P(\tau; \theta)R(\tau)
\end{equation}
\subparagraph{}In our new notation, our goal is to find $\theta$:
\begin{equation}
\max_{\theta}\eta(\theta) = \max_{\theta}\sum_{\tau}P(\tau; \theta)R(\tau)
\end{equation}
 

\begin{algorithm}
	\caption{REINFORCE: A Monte-Carlo Policy-Gradient Method (episodic)}
	\begin{algorithmic}
		\Require a differentiable policy parameterization $\pi(a|s, \theta)$, $\forall a \in A, s \in S, \theta \in R^{n}$
		\State Initialize policy weights $\theta$
		\While{True}
		\State Generate an episode $\{S_0, A_0, R_1, S_1, A_1, R_2, ..., S_{T-1}, A_{T-1}, R_T, S_T\}$ follow $\pi(\cdot | \cdot, \theta)$
		\For{each step of the episode $t=0, 1, ..., T-1$}
			\State $G_t \leftarrow $ return from step $t$
			\State $\theta \leftarrow \theta + \alpha \gamma^{t} G_t \nabla_{\theta}log\pi(A_{t}|S_{t}, \theta)$
		\EndFor
		\EndWhile
	\end{algorithmic}
\end{algorithm}
\end{document}