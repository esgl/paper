\ifx\allfiles\undefined
\documentclass[12pt,a4paper]{article}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{geometry}
\usepackage{indentfirst}
\usepackage{pifont}
\usepackage{ulem}
\usepackage{color}
\usepackage{algorithm} 
\usepackage{algorithmicx} 
\usepackage{algpseudocode}  
\usepackage{amsmath}  
\renewcommand{\algorithmicrequire}{\textbf{Input:}}  % Use Input in the format of Algorithm  
\renewcommand{\algorithmicensure}{\textbf{Output:}} % Use Output in the format of Algorithm  

\setlength{\parindent}{0em}
\geometry{left=2.0cm,right=2.0cm,top=2.5cm,bottom=2.5cm}

\begin{document}
\title{Neural K-Nearest Neibhorhood Training}
\author{Guannan Hu}
\maketitle
\fi
\section{Introduction}
\paragraph{} Until the AlphaGo defeated the 18-time world champion Lee Sedol, the reinforcement learning have a rapid development with deep neural network. such as play atari game without human knowledge \cite{Mnih2013Playing},  and AlphaZero \cite{silver2017mastering} who is the extension version have been trained without human knowledge and self-play. Although the deep reinforcement learning have a great performance, there exist a lot of problems in deep reinforcement learning algorithms.
\begin{itemize}
	\item The algorithms must able to learn from a scalar reward signal that is frequently sparse, noisy and delayed. The deplay between actions and resulting rewards, which can be thousands of time-steps long, seems particularly daunting when compared to the direct association between inputs and targets found in supervised learning;
	\item The most deep learning algorithms assume the data samples to be independent, while in reinforcement learning one typically encounters sequences of highly correlated states;
	\item In RL, the data distribution changes as the algorithm learns new behaviours, which can be problematic for deep learning methods that assume a fixed underlying distribution;
	\item Stochastic gradient optimisation requires the use of small learning rates. Due to the global approximation nature of neural networks, high learning rates cause catastrophic interference. Low learning rates mean that experience can only be incorporated into a neural network slowly;
	\item Environments with sparse reward signal can be difficult for a neural network to model as there may be very few instances where the reward is non-zero. This can be viewed as a form of class imbalance where low-reward samples outnumber high-reward samples by a unknown number. Consequently, the neural network disproportionally underperforms at predicting larger rewards, making it difficult for an agent to take the most rewarding actions;
	\item Reward signal propagation by value-bootstrappiDeep Q-Learning Networkng techniques, such as Q-learning, results in reward information being propagated one step at a time through the history of previous interactions with the environment. This can be fairly efficient if updates happen in reverse order in which the transitions occur. However, in order to train on randomly selected transitions, and, in order to further stabilise training, required the use of slow updating \textit{target network} further slowing down reward propagation.
\end{itemize} 
\paragraph{Deep Q-learning Network}  Mnih et.al present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning.\cite{silver2016mastering} The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards.
\paragraph{Double Deep Q-learning Network} Double Deep Q-Learning Network
\paragraph{Dueling Deep Q-learning Network} Dueling Deep Q-Learning Network
\paragraph{Experience Replay and Prioritized Experience Replay} 
\paragraph{Trusted Region Policy Optimization} Trust Region Policy Optimization
\paragraph{Proximity Policy Optimization} Proximity Policy Optimization
\paragraph{Neural Episodic Training} \cite{Pritzel2017Neural}

\ifx\allfiles\undefined
\bibliographystyle{plain}
\bibliography{reference}
\end{document}
\fi