\paragraph{Distributed Important Sampling} A complementary family of techniques for speeding up training is based on variance reduction by means of \textit{\uline{importance sampling}}. This has been shown be useful in the context of neural networks. Sampling non-uniformly from a dataset and weighting updates according to the sampling probability in order to counteract the bias thereby introduced can increase the speed of convergence by reducing the variance of the gradients. One way of doing this is to select samples with probability proportional to the $L_{2}$ norm of the corresponding gradients. In supervised learning, this approach has been successfully extended to the distributed setting. An alternative is to rank samples according to their latest known loss value and make the sampling probability a function of the rank rather than of the loss itself.