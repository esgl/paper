\paragraph{Trust Region Policy Optimization, TRPO} In Trust Region Policy Optimization \cite{Schulman2015Trust, Wu2017Scalable}, an objective function(the "surrogate" objective) is maximized subject to a constraint on the size of the policy update. Specifically,
\begin{equation}
\begin{aligned}
\max_{\theta} \ \hat{\mathbb{E}}_{t}\left[\frac{\pi_{\theta}(a_{t}|s_{t})}{\pi_{old}(a_{t}|s_{t})}\hat{A}_{t}\right]\\
\text{subject to} \ \hat{\mathbb{E}}_{t}[\text{KL}[\pi_{\theta_{old}}(\cdot|s_{t}), \pi_{\theta}(\cdot|s_{t})]] \leq \delta
\end{aligned}
\end{equation}
Here,$\theta_{old}$ is the vector of policy parameters before the update. This problem can efficiently approximately solved using the conjugate algorithm, after making a linear approximation to the objective and a quadratic approximation to the constraint.
\paragraph{}The theory justifying TRPO actually suggests using a penalty instead of a constraint, i.e., solving the unconstrained optimization problem
\begin{equation}
\max_{\theta}\hat{\mathbb{E}}_{t}\left[\frac{\pi_{\theta}(a_{t}|s_{t})}{\pi_{\theta_{old}}(a_{t}|s_{t})}\hat{A}_{t} - \beta \text{KL}[\pi_{old}(\cdot|s_{t}), \pi_{\theta}(\cdot|s_{t})] \right]
\end{equation}
for some coefficient $\beta$. This follows from the fact that a certain surrogate objective (which computes the max KL over states instead of the mean) forms a lower bound (i.e., a pessimistic bound) on the performance of the policy $\pi$. TRPO uses a hard constraint rather than a penalty because it is hard to choose a single value of $\beta$ that performs well across different problem -- or even within a single problem, where the characteristics change over the course of learning. Hence, to achieve our goal of the first-order algorithm that emulates the monotonic improvement of TRPO.
\paragraph{Clipped Surrogate Objective} Let $r_{t}(\theta)$ denote the probability ratio $r_{t}(\theta)=\frac{\pi_{\theta}(a_{t}|s_{t})}{\pi_{\theta_{old}}(a_{t}|s_{t})}$, so $r(\theta_{old})=1$. TRPO maximizes a "surrogate" objective 
\begin{equation}
L^{CPI}(\theta) = \hat{\mathbb{E}}\left[\frac{\pi_{\theta}(a_{t}|s_{t})}{\pi_{\theta_{old}}(a_{t}|s_{t})}\hat{A}_{t}\right] =\hat{\mathbb{E}}_{t}[r_{t}(\theta)\hat{A}_{t}]
\end{equation}
The superscript $CPI$ refers to conservative policy iteration,where this objective was proposed. \uline{Without a constraint, maximization of $L^{CPI}$ would lead an excessively large policy update;} hence, we now consider how to modify the objective, to penalize changes to the policy that move $r_{t}(\theta)$ away from 1.
\paragraph{}The main object we propose is the following:
\begin{equation}
L^{CLIP}(\theta) = \hat{\mathbb{E}}[\text{min}(r_{t}(\theta)\hat{A}_{t}, \text{clip}(r_{t}(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_{t})]
\end{equation}
The motivation for this objective is as follows. \uline{The first term inside the min is $L^{CPI}$. the second term, $\text{clip}(r_{t}(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_{t}$, modifies the surrogate objective by clipping the probability ratio, which removes the incentive for moving $r_{t}$ outside of the interval $[1-\epsilon, 1+\epsilon]$. Finally, we take the minimum of the clipped and unclipped objective, so the final objective is lower bound on the unclipped objective.With this scheme, we only ignore the change in probability ratio when it would make the objective improve and include it when it makes the objective worse. Note that $L^{CLIP}(\theta) = L^{CPI}(\theta)$ to first order around $\theta_{old}$, however, they become different as $\theta$ moves away from $\theta_{old}$.}

\paragraph{Adaptive KL Penalty Coefficient}
Another approach, which can be used as an alternative to the clipped surrogate objective, or in addition to it, is to use a penalty on KL divergence, and to adapt the penalty coefficient so that we achieve some target value of the KL divergence $d_{targ}$ each policy update. \textcolor{red}{\uline{In the experiments, it found that the KL penalty performed worse than the clipped surrogate objective, however, we've included it here because it's an important baseline.}}
\paragraph{}In the simplest instantiation of this algorithm, we perform the following steps in each policy update:
\begin{itemize}
\item Using several epochs of minibatch SGD, optimize the KL-penalized objective
\begin{equation}
L^{KLPEN}(\theta)=\hat{\mathbb{E}}\left[\frac{\pi_{\theta}(a_{t}|s_{t})}{\pi_{\theta_{old}}(a_{t}|s_{t})}\hat{A}_{t}-\beta\text{KL}[\pi_{\theta_{old}}(\cdot|s_{t}),\pi_{\theta}(\cdot|s_{t})]\right]
\end{equation}
\item Compute $d=\hat{\mathbb{E}}_{t}[\text{KL}[\pi_{\theta_{old}}(\cdot|s_{t}), \pi_{\theta}(\cdot|s_{t})]]$
	\begin{itemize}
		\item[-] If $d < d_{targ}/1.5, \beta \leftarrow \beta/2$
		\item[-] If $d > d_{targ} \times 1.5, \beta \leftarrow \beta \times 2$
	\end{itemize}
\end{itemize}
The updated $\beta$ is used for the next policy update. With this scheme, we occasionally see policy updates where the KL divergence is significantly different from $d_{targ}$, however, there are rare, and $\beta$ quickly adjusts. The parameters 1.5 and 2 above chosen heuristically.