\paragraph{Experience Replay and Prioritized Experience Replay} Experience Replay  \cite{wang2016sample} has gained popularity in DQN, where it it often motivated as a technique for reducing sample correlation. Experience Replay addresses both of these issue: with experience stored in a replay memory, it becomes possible to break the temporal correlations by mixing more and less recent experience for the updates, and rare experience will be used for more than just a single update. In general, experience replay can reduce the amount of experience required to learn, and replace it with more computation and more memory - which are often cheaper resources that the RL's agent's interactions with its environment. In Experience Replay, the experience transitions were uniformly sampled from a replay memory. the transition are replayed at the same frequency that they were original experienced, regardless of their significance.
and in Prioritized Experience Replay \cite{schaul2015prioritized}, so as to replay important transitions more frequently, and therefore learn more efficiently. and the distributed prioritized experience relpay \cite{horgan2018distributed} the algorithm decouples acting from learning: the actors interact with their own instance of the environment by selecting actions according to a shared neural network, and accumulate the resulting experience in a shared experience replay memory; the learner replays samples of experience and updates the neural network. 